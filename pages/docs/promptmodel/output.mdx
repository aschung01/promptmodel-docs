# Output

## Format

All of **promptmodel**'s methods return `LLMResponse` or `LLMStreamResponse` objects.

### LLMResponse

Here is the exact json output and type of `LLMResponse`.

```python
{
	'api_response' : {
		'choices': [
			{
				'finish_reason': Optional[str],     # String: 'stop' | None
				'index': int,             			# Integer: 0
				'message': {              			# Dictionary [str, str]
					'role': str,            		# String: 'assistant'
					'content': str          		# String: "default message"
				}
			}
		],
		'created': str,               				# String
		'model': str,                 				# String
		'usage': {                    				# Dictionary [str, int]
			'prompt_tokens': int,       			# Integer
			'completion_tokens': int,   			# Integer
			'total_tokens': int         			# Integer
		},
		'response_ms' : float 						# float
	},
	'raw_output' : Optional[str],
	'parsed_outputs' : Optional[Dict[str, str]],
	'function_call' : Optional[Dict[str, Any]],
	'error' : Optional[bool],
	'error_log' : Optional[str]
}
```

You can use `LLMResponse.api_response` as same as response of openai API response.
For example, the codes below return same output.

```python
import openai
from promptmodel import PromptModel


openai_response = openai.ChatCompletion.create(
	model="gpt-3.5-turbo",
	messages=[{"role" : "system", "content" : ""}, {"role" : "user", "content" : "Say Hello"}]
)
print(openai_response['choices'][0]['messages']['content'])
# Hello

promptmodel_response = PromptModel("say_hello").run()
print(promptmodel_response.api_response['choices'][0]['messages']['content'])
# Hello
```

You can also use `LLMResponse.raw_output` to get raw output of openai API response.

```python
promptmodel_response = PromptModel("say_hello").run()
print(promptmodel_response.raw_output)
# Hello
```

If you use parsing methods like `run_and_parse`, you can get parsed output with `LLMResponse.parsed_outputs`.

```python
promptmodel_response = PromptModel("say_hello").run_and_parse()
print(promptmodel_response.parsed_outputs)
# {"think" : "I will say Hello", "response" : "Hello"}
```

### LLMStreamResponse

Streaming methods like `stream()` or `stream_and_parse()` return **LLMStreamResponse**.

Here is the exact json output and type of `LLMStreamResponse`.

```python
{
	'api_response' : {
		'choices': [
			{
				'finish_reason': Optional[str],     # String: None
				'index': int,             			# Integer: 0
				'delta': {              			# Dictionary [str, str]
					'role': str,            		# String: 'assistant'
					'content': str          		# String: "default message"
				}
			}
		],
		'created': str,              				# String
		'model': str,                 				# String
		'usage': {                    				# Dictionary [str, int]
			'prompt_tokens': int,       			# Integer
			'completion_tokens': int,   			# Integer
			'total_tokens': int         			# Integer
		},
		'response_ms' : float 						# float
	},
	'raw_output' : Optional[str],
	'parsed_outputs' Optional[Dict[str, str]],
	'function_call' : Optional[Dict[str, Any]],
	'error' : Optional[bool],
	'error_log' : Optional[str]
}
```

You can use `LLMStreamResponse.api_response` as same as response of openai API stream response.
For example, the codes below return same output.

```python
import openai
from promptmodel import PromptModel


openai_response = openai.ChatCompletion.create(
	model="gpt-3.5-turbo",
	messages=[{"role" : "system", "content" : ""}, {"role" : "user", "content" : "Say Hello"}],
	stream=True
)
for chunk in openai_response:
	print(chunk['choices'][0]['delta']['content'])
# Hello

promptmodel_response = PromptModel("say_hello").stream()
for chunk in promptmodel_response:
	print(chunk.api_response['choices'][0]['delta']['content'])
# Hello
```

You can also use `LLMStreamResponse.raw_output` to get raw output of openai API stream response.

```python
promptmodel_response = PromptModel("say_hello").stream()
for chunk in promptmodel_response:
	print(chunk.raw_output)
# Hello
```

You can also use `LLMStreamResponse.parsed_outputs` to get parsed output of openai API stream response.
LLM response will be streamed as Dictionary.

```python
promptmodel_response = PromptModel("say_hello").stream_and_parse()
for chunk in promptmodel_response:
	print(chunk.parsed_outputs)
	# {"think" : "I"}
	# {"think" : "will"}
	# {"think" : "say"}
	# {"think" : "Hello."}
	# {"response" : "Hello"}
```
